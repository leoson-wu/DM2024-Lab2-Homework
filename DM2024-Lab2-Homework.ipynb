{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 吳征彥\n",
    "\n",
    "Student ID: 113062636\n",
    "\n",
    "GitHub ID: https://github.com/leoson-wu\n",
    "\n",
    "Kaggle name: Leoson Wu\n",
    "\n",
    "Kaggle private scoreboard snapshot:  \n",
    "![Leaderboard](leaderboard.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-03T07:43:07.334527Z",
     "iopub.status.busy": "2024-12-03T07:43:07.334180Z",
     "iopub.status.idle": "2024-12-03T07:43:41.912246Z",
     "shell.execute_reply": "2024-12-03T07:43:41.911536Z",
     "shell.execute_reply.started": "2024-12-03T07:43:07.334497Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# import package\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T07:43:41.914198Z",
     "iopub.status.busy": "2024-12-03T07:43:41.913662Z",
     "iopub.status.idle": "2024-12-03T07:43:49.409031Z",
     "shell.execute_reply": "2024-12-03T07:43:49.408321Z",
     "shell.execute_reply.started": "2024-12-03T07:43:41.914171Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# Load tweets_df pickel file\n",
    "tweets_df = pd.read_pickle(\"/kaggle/input/df-pickel/raw.pkl\")\n",
    "# Load the CSV files into DataFrames\n",
    "data_path = '/kaggle/input/dm-2024-isa-5810-lab-2-homework'\n",
    "submission_df = pd.read_csv(os.path.join(data_path, 'sampleSubmission.csv'))\n",
    "data_identification_df = pd.read_csv(os.path.join(data_path, 'data_identification.csv'))\n",
    "emotion_df = pd.read_csv(os.path.join(data_path, 'emotion.csv'))\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T07:50:12.113666Z",
     "iopub.status.busy": "2024-12-03T07:50:12.113307Z",
     "iopub.status.idle": "2024-12-03T07:50:12.321972Z",
     "shell.execute_reply": "2024-12-03T07:50:12.321122Z",
     "shell.execute_reply.started": "2024-12-03T07:50:12.113637Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# Look into the distribution of _score\n",
    "emotion_scores_distribution = tweets_df.groupby('emotion')['_score'].describe()\n",
    "# Display the result\n",
    "print(emotion_scores_distribution)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T07:52:50.756510Z",
     "iopub.status.busy": "2024-12-03T07:52:50.755938Z",
     "iopub.status.idle": "2024-12-03T07:57:14.548665Z",
     "shell.execute_reply": "2024-12-03T07:57:14.547701Z",
     "shell.execute_reply.started": "2024-12-03T07:52:50.756477Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# Preprocessing steps\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Convert text to lowercase\n",
    "tweets_df['text'] = tweets_df['text'].str.lower()\n",
    "\n",
    "# Remove URLs from text\n",
    "tweets_df['text'] = tweets_df['text'].str.replace(r'http\\S+|www\\S+', '', regex=True)\n",
    "\n",
    "# Function to extract hashtags and clean text\n",
    "def extract_hashtags(text):\n",
    "    # Extract hashtags (without the '#' symbol)\n",
    "    hashtags = re.findall(r'#(\\w+)', text)\n",
    "    # Remove '#' from hashtags in the text\n",
    "    updated_text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    return updated_text, hashtags\n",
    "\n",
    "# Apply the function and update the 'text' and 'hashtags' columns\n",
    "tweets_df[['text', 'hashtags']] = tweets_df.apply(\n",
    "    lambda row: pd.Series(extract_hashtags(row['text'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Ensure 'hashtags' column contains empty lists instead of NaN\n",
    "tweets_df['hashtags'] = tweets_df['hashtags'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Clean text further: remove splaceholder and repeated punctuation marks\n",
    "tweets_df['text'] = tweets_df['text'].apply(lambda x: re.sub(r'<lh>', '', x).strip()) \n",
    "tweets_df['text'] = tweets_df['text'].apply(lambda x: re.sub(r'[!.,?]+', '', x)) \n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T08:23:34.355619Z",
     "iopub.status.busy": "2024-12-03T08:23:34.355255Z",
     "iopub.status.idle": "2024-12-03T08:23:35.048563Z",
     "shell.execute_reply": "2024-12-03T08:23:35.047857Z",
     "shell.execute_reply.started": "2024-12-03T08:23:34.355591Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# trainning data selection (sampling) \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "tweets_test = tweets_df[tweets_df['emotion'] == 'test']\n",
    "tweets_train = tweets_df[tweets_df['emotion'] != 'test']\n",
    "tweets_train = tweets_train[tweets_df['_score'] > 769] # use top 25% _score dataset for training \n",
    "tweets_train = tweets_train.sample(frac=0.08, random_state=42)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T08:23:36.152210Z",
     "iopub.status.busy": "2024-12-03T08:23:36.151515Z",
     "iopub.status.idle": "2024-12-03T08:23:36.265765Z",
     "shell.execute_reply": "2024-12-03T08:23:36.264874Z",
     "shell.execute_reply.started": "2024-12-03T08:23:36.152177Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# Examine the sample dataset\n",
    "# Checking we include considerable training data for each emotion \n",
    "sample_emotion_distribution = tweets_train.groupby('emotion')['_score'].describe()\n",
    "print(sample_emotion_distribution)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T08:23:40.578967Z",
     "iopub.status.busy": "2024-12-03T08:23:40.578143Z",
     "iopub.status.idle": "2024-12-03T08:25:54.986739Z",
     "shell.execute_reply": "2024-12-03T08:25:54.985872Z",
     "shell.execute_reply.started": "2024-12-03T08:23:40.578933Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# Create a mapping from emotion labels to integers\n",
    "emotions = tweets_train['emotion'].unique()\n",
    "label_map = {label: idx for idx, label in enumerate(emotions)}\n",
    "# Apply mapping to the emotion column\n",
    "tweets_train['label'] = tweets_train['emotion'].map(label_map)\n",
    "tweets_train = tweets_train[['tweet_id', 'text', 'label']]\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "```python\n",
    "# Use robertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "    \n",
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(tweets_train)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T08:25:55.014919Z",
     "iopub.status.busy": "2024-12-03T08:25:55.014616Z",
     "iopub.status.idle": "2024-12-03T08:25:55.189991Z",
     "shell.execute_reply": "2024-12-03T08:25:55.189332Z",
     "shell.execute_reply.started": "2024-12-03T08:25:55.014883Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# prepare model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_map))\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T08:25:55.191638Z",
     "iopub.status.busy": "2024-12-03T08:25:55.191371Z",
     "iopub.status.idle": "2024-12-03T08:25:55.227877Z",
     "shell.execute_reply": "2024-12-03T08:25:55.227078Z",
     "shell.execute_reply.started": "2024-12-03T08:25:55.191612Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# setting training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"no\",  # Disables evaluation during training because we don't have test data for validation.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T08:26:01.152505Z",
     "iopub.status.busy": "2024-12-03T08:26:01.151935Z"
    },
    "trusted": true
   },
   "source": [
    "```python\n",
    "# start to train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "```python\n",
    "# define a function to predict emotions from the input text  \n",
    "def predict_emotions(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    return [emotions[pred] for pred in predictions]\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "```python\n",
    "# evoke the GPU\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Ensure model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# batch the input data to avoid memory issue\n",
    "batch_size = 16\n",
    "predictions = []\n",
    "\n",
    "# do the classification\n",
    "for i in tqdm(range(0, len(tweets_test), batch_size)):\n",
    "    batch_texts = tweets_test['text'][i:i + batch_size].tolist()\n",
    "    \n",
    "    # Tokenize and move to the correct device\n",
    "    inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get the predictions\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        batch_predictions = model(**inputs).logits.argmax(dim=-1).cpu().tolist()  # Move the result back to CPU if necessary\n",
    "    \n",
    "    predictions.extend(batch_predictions)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "```python\n",
    "# append the final result/answers\n",
    "tweets_test['emotion'] = predictions\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "```python\n",
    "# join the predicted result to the submission.csv  \n",
    "submission_df.drop(columns='emotion')\n",
    "submission_df = submission_df.merge(tweets_test[['tweet_id', 'emotion']], left_on='id', \n",
    "    right_on='tweet_id', how='left')\n",
    "submission_df['emotion'] = submission_df['emotion_y'].apply(lambda x: emotions[x])\n",
    "submission_df = submission_df[['id', 'emotion']]\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "```python\n",
    "# save to the submission file\n",
    "submission_df.to_csv('/kaggle/working/submission.csv', index=False, header=True)\n",
    "```\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9912598,
     "sourceId": 87232,
     "sourceType": "competition"
    },
    {
     "datasetId": 6117860,
     "sourceId": 9948691,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6153819,
     "sourceId": 9998172,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
